# üß™ Guide de Tests - Render Signal Server

Ce document d√©crit la strat√©gie de tests compl√®te pour le projet, couvrant les tests unitaires, d'int√©gration et end-to-end, ainsi que les bonnes pratiques et les m√©triques de qualit√©.

## üìã Table des Mati√®res

- [üìä Vue d'ensemble](#-vue-densemble)
- [‚öôÔ∏è Installation](#-installation)
- [üöÄ Ex√©cution des tests](#-ex√©cution-des-tests)
- [üß© Types de tests](#-types-de-tests)
- [üìà Couverture de code](#-couverture-de-code)
- [üéØ Bonnes pratiques](#-bonnes-pratiques)
- [üîÑ CI/CD](#-cicd)
- [üîç D√©pannage](#-d√©pannage)
- [üìä M√©triques de qualit√©](#-m√©triques-de-qualit√©)

## üìä Vue d'ensemble

Le projet utilise **pytest** comme framework de test principal, avec une suite d'outils modernes pour assurer une couverture maximale et une d√©tection pr√©coce des r√©gressions.

### üß∞ Outils Principaux

- **pytest** - Framework de test principal
- **pytest-cov** - Analyse de couverture de code
- **pytest-mock** - Cr√©ation de mocks et de stubs
- **pytest-flask** - Int√©gration avec Flask
- **fakeredis** - Mock Redis pour les tests
- **freezegun** - Contr√¥le du temps dans les tests
- **responses** - Mock des requ√™tes HTTP
- **hypothesis** - Tests bas√©s sur la propri√©t√©

### üèóÔ∏è Structure des Tests

```
render_signal_server-main/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/                  # Tests unitaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/          # Tests des services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Tests des utilitaires
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth/              # Tests d'authentification
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ integration/           # Tests d'int√©gration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/               # Tests des routes API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/          # Tests d'int√©gration des services
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ e2e/                   # Tests end-to-end
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ email_processing/  # Flux complets de traitement d'emails
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ webhooks/          # Tests des webhooks
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/              # Donn√©es de test
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py            # Configuration et fixtures partag√©es
‚îÇ   ‚îî‚îÄ‚îÄ helpers/               # Utilitaires de test
‚îÇ
‚îú‚îÄ‚îÄ pytest.ini                 # Configuration pytest
‚îî‚îÄ‚îÄ .coveragerc                # Configuration de la couverture
```

### üìä M√©triques Cl√©s

- **Couverture de code** : 67.3% (objectif : 80%+)
- **Tests passants** : 282/290 (97.2%)
- **Temps d'ex√©cution** : ~45s (sans les tests lents)
- **Derni√®re ex√©cution** : 2025-11-18 14:30:45

## ‚öôÔ∏è Installation

### Pr√©requis

- Python 3.9+
- pip 20.0+
- Redis (optionnel, pour les tests d'int√©gration complets)

### Installation des D√©pendances

```bash
# Cloner le d√©p√¥t
git clone https://github.com/votre-utilisateur/render_signal_server.git
cd render_signal_server

# Cr√©er un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Sur Windows: .\venv\Scripts\activate

# Installer les d√©pendances de d√©veloppement
pip install -r requirements-dev.txt
```

### Configuration de l'Environnement

Cr√©ez un fichier `.env.test` √† la racine du projet avec les variables d'environnement n√©cessaires :

```env
FLASK_ENV=testing
FLASK_APP=app_render.py
TESTING=True
DISABLE_BACKGROUND_TASKS=true
```

### D√©pendances de Test

| Package | Version | Description |
|---------|---------|-------------|
| pytest | >=7.0 | Framework de test principal |
| pytest-cov | >=4.0 | Couverture de code |
| pytest-mock | >=3.10 | Mocks et stubs |
| pytest-flask | >=1.2 | Int√©gration Flask |
| fakeredis | >=2.10 | Mock Redis |
| freezegun | >=1.2 | Contr√¥le du temps |
| responses | >=0.23 | Mock HTTP |
| hypothesis | >=6.0 | Tests bas√©s sur la propri√©t√© |
| black | >=22.0 | Formatage du code |
| flake8 | >=4.0 | Linting |
| mypy | >=0.9 | V√©rification des types |
| safety | >=2.0 | V√©rification des vuln√©rabilit√©s |

## üöÄ Ex√©cution des Tests

### üéØ Ex√©cution de Base

```bash
# Tous les tests avec couverture
pytest --cov=.

# Avec plus de d√©tails
pytest -v

# Afficher les sorties de d√©bogage
pytest -s

# Arr√™ter au premier √©chec
pytest -x

# Ex√©cuter uniquement les tests qui ont √©chou√© lors de la derni√®re ex√©cution
pytest --last-failed
```

### üß© Tests par Cat√©gorie

```bash
# Tests unitaires
pytest -m unit

# Tests d'int√©gration
pytest -m integration

# Tests end-to-end
pytest -m e2e

# Exclure les tests lents
pytest -m "not slow"

# Exclure les tests n√©cessitant Redis
pytest -m "not redis"

# Exclure les tests n√©cessitant IMAP
pytest -m "not imap"

# Ex√©cuter les tests marqu√©s comme critiques
pytest -m "critical"
```

### üìÇ Tests par Module ou Classe

```bash
# Tous les tests d'un module
pytest tests/unit/services/test_config_service.py

# Tous les tests d'une classe
pytest tests/unit/services/test_config_service.py::TestConfigService

# Un test sp√©cifique
pytest tests/unit/services/test_config_service.py::TestConfigService::test_get_setting
```

### üîç Filtrage des Tests

```bash
# Par nom de test
pytest -k "config"

# Exclure certains tests
pytest -k "not slow and not integration"

# Afficher les tests les plus lents
pytest --durations=10
```

### ‚ö° Ex√©cution Parall√®le

```bash
# D√©tection automatique du nombre de c≈ìurs
pytest -n auto

# Sp√©cifier le nombre de workers
pytest -n 4

# Mode fail-fast en parall√®le
pytest -n auto --dist=loadscope -x
```

### üìä Rapports de Couverture

```bash
# G√©n√©rer un rapport HTML
pytest --cov=. --cov-report=html

# Afficher les parties non couvertes
pytest --cov=. --cov-report=term-missing

# D√©finir un seuil minimum de couverture (√©chec si en dessous)
pytest --cov=. --cov-fail-under=80
```

## üß© Types de Tests

### üß™ Tests Unitaires (`@pytest.mark.unit`)

Tests isol√©s d'une seule unit√© de code (fonction, m√©thode, classe) sans d√©pendances externes.

**Objectif** : V√©rifier le comportement d'une unit√© de code de mani√®re isol√©e.

**Exemple :**
```python
@pytest.mark.unit
def test_normalize_text():
    """Teste la normalisation du texte (suppression des accents, minuscules)."""
    result = normalize_no_accents_lower_trim("Caf√© √âl√©gant")
    assert result == "cafe elegant"
```

**Couverture Actuelle :**
- `utils/` : Fonctions utilitaires (time_helpers, text_helpers, validators)
- `auth/` : Gestion de l'authentification (user, helpers)
- `email_processing/` : Traitement des emails (pattern_matching, link_extraction, payloads)
- `services/` :
  - `ConfigService` : Gestion de la configuration
  - `RuntimeFlagsService` : Gestion des flags runtime
  - `WebhookConfigService` : Configuration des webhooks
  - `DeduplicationService` : Pr√©vention des doublons
  - `AuthService` : Authentification et autorisation
  - `PollingConfigService` : Configuration du polling
  - `tests/test_absence_pause.py` : v√©rifie la normalisation des jours (`strip().lower()`) et la garde de cycle (`ABSENCE_PAUSE`), garantissant que le poller s'arr√™te avant toute connexion IMAP les jours d'absence.

### üîÑ Tests d'Int√©gration (`@pytest.mark.integration`)

Tests v√©rifiant l'interaction entre plusieurs composants ou modules.

**Objectif** : V√©rifier que les composants fonctionnent correctement ensemble.

**Exemple :**
```python
@pytest.mark.integration
def test_webhook_config_persistence(authenticated_client, temp_file):
    """Teste le cycle complet de persistance de la configuration des webhooks."""
    # 1. Cr√©ation d'une configuration
    config_data = {
        "webhook_url": "https://api.example.com/webhook",
        "enabled": True,
        "timeout": 30
    }
    
    # 2. Enregistrement de la configuration
    response = authenticated_client.post(
        '/api/webhooks/config',
        json=config_data,
        headers={"Content-Type": "application/json"}
    )
    assert response.status_code == 200
    
    # 3. R√©cup√©ration de la configuration
    response = authenticated_client.get('/api/webhooks/config')
    assert response.status_code == 200
    
    # 4. V√©rification des donn√©es
    saved_config = response.json['config']
    assert saved_config['webhook_url'] == config_data['webhook_url']
    assert saved_config['enabled'] == config_data['enabled']
    assert saved_config['timeout'] == config_data['timeout']
```

**Couverture Actuelle :**
- Routes API (`routes/`)
- Int√©gration des services
- Persistance des donn√©es (JSON, Redis)
- Validation des entr√©es/sorties
- Gestion des erreurs

### üåê Tests End-to-End (`@pytest.mark.e2e`)

Tests du flux complet de bout en bout, simulant un utilisateur r√©el.

**Objectif** : V√©rifier que l'ensemble du syst√®me fonctionne comme pr√©vu.

**Exemple :**
```python
@pytest.mark.e2e
def test_complete_email_processing_flow(
    imap_server_mock, 
    webhook_receiver,
    test_email_with_dropbox_links
):
    """Teste le flux complet de r√©ception et traitement d'un email."""
    # 1. Configuration du mock IMAP pour retourner un email de test
    imap_server_mock.add_email(test_email_with_dropbox_links)
    
    # 2. D√©clenchement du traitement
    response = test_client.post('/api/check_emails_and_download')
    assert response.status_code == 202
    
    # 3. V√©rification que le webhook a √©t√© appel√©
    webhook_receiver.wait_for_request(timeout=5.0)
    
    # 4. V√©rification du contenu du webhook
    request = webhook_receiver.get_latest_request()
    payload = request.get_json()
    
    assert payload['subject'] == test_email_with_dropbox_links['subject']
    assert len(payload['links']) > 0
    assert payload['detector'] == 'media_solution'
```

**Couverture Actuelle :**
- Flux complet de traitement des emails
- Sc√©narios M√©dia Solution
- Sc√©narios DESABO (urgent/non urgent)
- Gestion des erreurs et reprises
- Notifications et webhooks

### üé≤ Tests de Propri√©t√© (`@pytest.mark.property`)

Tests bas√©s sur des propri√©t√©s avec g√©n√©ration al√©atoire de donn√©es.

**Objectif** : D√©tecter les cas limites et les erreurs subtiles.

**Exemple :**
```python
from hypothesis import given, strategies as st

@given(
    text=st.text(
        alphabet=st.characters(
            min_codepoint=1,
            max_codepoint=1000,
            blacklist_categories=('Cc', 'Cs')
        ),
        min_size=1
    )
)
@pytest.mark.property
def test_normalize_no_accents_property(text):
    """Teste que la normalisation conserve la longueur du texte."""
    normalized = normalize_no_accents(text)
    assert len(normalized) == len(text)
```

**Couverture Actuelle :**
- Fonctions de manipulation de texte
- Validation des entr√©es
- Conversion de formats
- Gestion des cas limites

### üè∑Ô∏è Marqueurs de Test

Le projet utilise des marqueurs pour cat√©goriser et contr√¥ler l'ex√©cution des tests.

#### Marqueurs Int√©gr√©s

- `@pytest.mark.slow` : Tests prenant plus de temps √† s'ex√©cuter
  ```bash
  # Exclure les tests lents
  pytest -m "not slow"
  ```

- `@pytest.mark.redis` : Tests n√©cessitant Redis
  ```bash
  # Ex√©cuter uniquement les tests Redis
  pytest -m "redis"
  
  # Exclure les tests Redis
  pytest -m "not redis"
  ```

- `@pytest.mark.imap` : Tests n√©cessitant une connexion IMAP
  ```bash
  # Ex√©cuter les tests IMAP (n√©cessite une configuration valide)
  pytest -m "imap"
  ```

- `@pytest.mark.e2e` : Tests end-to-end
  ```bash
  # Ex√©cuter uniquement les tests E2E
  pytest -m "e2e"
  ```

#### Marqueurs Personnalis√©s

- `@pytest.mark.critical` : Tests critiques pour la validation des fonctionnalit√©s principales
  ```bash
  # Ex√©cuter uniquement les tests critiques
  pytest -m "critical"
  ```

- `@pytest.mark.flaky` : Tests sujets √† des √©checs intermittents
  ```bash
  # R√©essayer les tests √©chou√©s jusqu'√† 3 fois
  pytest --reruns 3 -m "flaky"
  ```

- `@pytest.mark.performance` : Tests de performance
  ```bash
  # Ex√©cuter les tests de performance
  pytest -m "performance"
  ```

### üß† Tests Param√©tr√©s

Utilisation de `@pytest.mark.parametrize` pour tester plusieurs sc√©narios avec des donn√©es diff√©rentes.

**Exemple :**
```python
@pytest.mark.parametrize("input_text,expected_output", [
    ("Hello World", "hello world"),
    ("TEST", "test"),
    ("MiXeD CaSe", "mixed case"),
    ("", ""),
    ("   trim   ", "trim"),
])
def test_normalize_text(input_text, expected_output):
    assert normalize_text(input_text) == expected_output
```

### üß™ Fixtures

Les fixtures sont d√©finies dans `tests/conftest.py` et peuvent √™tre utilis√©es dans tous les tests.

**Fixtures Principales :**
- `app` : Instance de l'application Flask
- `client` : Client de test Flask
- `db` : Base de donn√©es de test
- `redis_client` : Client Redis (ou mock)
- `imap_server_mock` : Mock du serveur IMAP
- `webhook_receiver` : Serveur de test pour recevoir les webhooks

**Exemple d'Utilisation :**
```python
def test_webhook_endpoint(client, webhook_receiver):
    # Configuration du webhook
    webhook_url = webhook_receiver.get_url()
    
    # Envoi d'une requ√™te au webhook
    response = client.post(
        "/api/webhooks",
        json={"message": "test"},
        headers={"Content-Type": "application/json"}
    )
    
    # V√©rification de la r√©ponse
    assert response.status_code == 200
    
    # V√©rification que le webhook a √©t√© appel√©
    webhook_receiver.wait_for_request()
    request = webhook_receiver.get_latest_request()
    assert request.get_json()["message"] == "test"
```

## üìà Couverture de Code

### G√©n√©ration des Rapports

```bash
# Rapport HTML interactif (ouvre le navigateur)
pytest --cov=. --cov-report=html

# Afficher les parties non couvertes dans le terminal
pytest --cov=. --cov-report=term-missing

# D√©finir un seuil minimum (√©chec si non atteint)
pytest --cov=. --cov-fail-under=70

# Combiner plusieurs formats de rapport
pytest --cov=. --cov-report=html --cov-report=term-missing

# Ouvrir le rapport HTML apr√®s g√©n√©ration
python -m webbrowser htmlcov/index.html  # Multiplateforme
# Rapport HTML interactif (ouvre le navigateur)
pytest --cov=. --cov-report=html

# Afficher les parties non couvertes dans le terminal
pytest --cov=. --cov-report=term-missing

# D√©finir un seuil minimum (√©chec si non atteint)
pytest --cov=. --cov-fail-under=70

# Combiner plusieurs formats de rapport
pytest --cov=. --cov-report=html --cov-report=term-missing

# Ouvrir le rapport HTML apr√®s g√©n√©ration
python -m webbrowser htmlcov/index.html  # Multiplateforme
```

### √âtat Actuel de la Couverture (2025-11-18)

| M√©trique | Valeur | Objectif |
|----------|--------|----------|
| Tests passants | 282/290 (97.2%) | 100% |
| Couverture globale | 67.3% | 75%+ |
| Temps d'ex√©cution | ~45s (hors tests lents) | < 2 min |

### Modules Cl√©s et Couverture

#### Services Principaux
- `services/runtime_flags_service.py` : 82% (cache TTL, invalidation, persistance)
- `services/webhook_config_service.py` : 78% (validation HTTPS, normalisation Make.com)
- `services/deduplication_service.py` : 75% (Redis + fallback m√©moire)
- `services/config_service.py` : 85% (gestion de la configuration)
- `services/auth_service.py` : 80% (authentification et autorisation)

#### Traitement des Emails
- `email_processing/orchestrator.py` : 72% (flux principal de traitement)
- `email_processing/pattern_matching.py` : 88% (d√©tection des mod√®les d'emails)
- `email_processing/link_extraction.py` : 85% (extraction des liens)

#### Routes API
- `routes/api_config.py` : 78% (gestion de la configuration)
- `routes/api_webhooks.py` : 75% (gestion des webhooks)
- `routes/api_logs.py` : 82% (journalisation et consultation des logs)

### Configuration de la Couverture

Le fichier `.coveragerc` d√©finit les exclusions et la configuration :

```ini
[run]
source = .
omit =
    /venv/*
    /tests/*
    */__pycache__/*
    */.pytest_cache/*
    */version.py
    */__init__.py

[report]
# Seuil d'√©chec (configur√© dans CI)
fail_under = 70

# Exclure les parties non pertinentes pour la couverture
exclude_lines =
    pragma: no cover
    def __repr__
    raise NotImplementedError
    if __name__ == .__main__.:
    pass
    raise ImportError
    @abc.abstractmethod
    @pytest.fixture
    @pytest.mark
    # Ignorer les m√©thodes magiques
    def __[a-zA-Z0-9_]+__
    # Ignorer les propri√©t√©s
    @property\s+def
    # Ignorer les setters
    @[a-zA-Z0-9_]+\.setter\s
```

### Strat√©gies d'Am√©lioration

#### 1. Analyse des Zones √† Am√©liorer

```bash
# Identifier les fichiers avec moins de 70% de couverture
pytest --cov=. --cov-report=term-missing | grep -v "100%" | sort -k4 -n

# G√©n√©rer un rapport HTML pour une analyse d√©taill√©e
pytest --cov=. --cov-report=html
```

#### 2. Exemples de Tests Manquants

**Fonction √† tester :**
```python
def calculate_discount(price, discount_percent):
    if discount_percent < 0 or discount_percent > 100:
        raise ValueError("Discount must be between 0 and 100")
    return price * (1 - discount_percent / 100)
```

**Tests manquants :**
```python
import pytest

@pytest.mark.parametrize("price, discount, expected", [
    (100, 10, 90.0),     # R√©duction de 10%
    (100, 0, 100.0),     # Aucune r√©duction
    (100, 100, 0.0),     # 100% de r√©duction
    (100, 50, 50.0),     # 50% de r√©duction
    (0, 10, 0.0),        # Prix √† z√©ro
])
def test_calculate_discount(price, discount, expected):
    assert calculate_discount(price, discount) == expected

# Tester les cas d'erreur
@pytest.mark.parametrize("discount", [-1, 101])
def test_calculate_discount_invalid(discount):
    with pytest.raises(ValueError):
        calculate_discount(100, discount)
```

#### 3. Exclure du Code D√©lib√©r√©ment

```python
# Exclure une fonction sp√©cifique
def experimental_feature():  # pragma: no cover
    """Fonction exp√©rimentale non encore test√©e."""
    pass

# Exclure un bloc de code
if DEBUG_MODE:  # pragma: no cover
    logger.warning("Mode d√©bogage activ√©")
```

### Int√©gration Continue

Configuration recommand√©e pour GitHub Actions (`.github/workflows/tests.yml`) :

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports: [6379:6379]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run tests with coverage
      env:
        REDIS_URL: redis://localhost:6379/0
      run: |
        pytest --cov=. --cov-report=xml --cov-fail-under=70
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        fail_ci_if_error: true
```

### Outils Recommand√©s

- **Codecov** : Suivi de la couverture dans le temps
- **SonarQube** : Analyse statique et m√©triques de qualit√©
- **Pylint** : V√©rification de la qualit√© du code
- **Black** : Formatage automatique du code
- **pre-commit** : Ex√©cution automatique des tests avant les commits

## üéØ Bonnes Pratiques de Test

### 1. Nommage des Tests

```python
# ‚úÖ Bon : descriptif et clair
def test_user_creation_with_valid_credentials():
    ...

def test_webhook_retry_on_failure():
    ...

# ‚ùå Mauvais : trop vague
def test_user():
    ...

def test_1():
    ...
```

### 2. Structure AAA (Arrange-Act-Assert)

```python
def test_webhook_config_update():
    # Arrange : pr√©parer les donn√©es
    config = {'webhook_url': 'https://example.com/hook'}
    
    # Act : ex√©cuter l'action
    result = update_webhook_config(config)
    
    # Assert : v√©rifier le r√©sultat
    assert result['status'] == 'success'
    assert result['url'] == config['webhook_url']
```

### 3. Utilisation de Fixtures

```python
import pytest

@pytest.fixture
def test_user():
    """Cr√©e un utilisateur de test avec des donn√©es par d√©faut."""
    return {
        'username': 'testuser',
        'email': 'test@example.com',
        'is_active': True
    }

def test_user_activation(test_user):
    # Utilisation de la fixture
    assert test_user['is_active'] is True
    
    # Test de la d√©sactivation
    test_user['is_active'] = False
    assert test_user['is_active'] is False
```

### 4. Tests Param√©tr√©s

```python
import pytest

@pytest.mark.parametrize("input_value,expected_output", [
    ("hello", "HELLO"),
    ("WORLD", "WORLD"),
    ("", ""),
    ("123", "123"),
])
def test_uppercase(input_value, expected_output):
    assert input_value.upper() == expected_output
```

## üîÑ CI/CD

### Configuration de Base

Le fichier `.github/workflows/tests.yml` est configur√© pour :
1. Ex√©cuter les tests sur chaque push et pull request
2. Tester sur Python 3.9 et 3.10
3. Utiliser Redis comme service pour les tests d'int√©gration
4. G√©n√©rer un rapport de couverture
5. Envoyer les r√©sultats √† Codecov

### D√©ploiement Automatique

Pour activer le d√©ploiement automatique apr√®s des tests r√©ussis :

```yaml
# Dans .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]
  workflow_run:
    workflows: ["Tests"]
    types: [completed]

jobs:
  deploy:
    if: github.ref == 'refs/heads/main' && github.event.workflow_run.conclusion == 'success'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      # √âtapes de d√©ploiement...
```

## üîç D√©pannage

### Tests √âchouant de Mani√®re Al√©atoire

1. **Probl√®me de concurrence** : Utilisez `@pytest.mark.flaky` pour les tests non d√©terministes
2. **Donn√©es partag√©es** : Assurez-vous que chaque test est isol√©
3. **D√©pendances externes** : Mockez les appels r√©seau et les bases de donn√©es

### Erreurs Courantes

```
# Erreur : Base de donn√©es verrouill√©e
# Solution : Assurez-vous de fermer correctement les connexions dans les fixtures

# Erreur : Timeout des tests
# Solution : Marquez les tests lents avec @pytest.mark.slow et ex√©cutez-les s√©par√©ment

# Erreur : √âchec de l'authentification
# Solution : V√©rifiez les jetons et les identifiants dans les variables d'environnement de test
```

## üìä M√©triques de Qualit√©

### Objectifs

- Couverture de code : 80% minimum
- Taux de r√©ussite des tests : 100%
- Temps d'ex√©cution total : < 2 minutes
- Nombre de tests unitaires > tests d'int√©gration > tests E2E

### Suivi

- Tableau de bord Codecov pour la couverture de code
- Rapports de tests GitHub Actions pour les √©checs
- M√©triques SonarQube pour la dette technique

## üìö Ressources

- [Documentation officielle de pytest](https://docs.pytest.org/)
- [pytest-cov documentation](https://pytest-cov.readthedocs.io/)
- [pytest-flask documentation](https://pytest-flask.readthedocs.io/)
- [Meilleures pratiques pour les tests Python](https://docs.pytest.org/en/stable/goodpractices.html)

---

*Derni√®re mise √† jour : 2025-11-18*

### Strat√©gies d'Am√©lioration

#### 1. Analyse des Zones √† Am√©liorer

```bash
# Identifier les fichiers avec moins de 70% de couverture
pytest --cov=. --cov-report=term-missing | grep -v "100%" | sort -k4 -n

# G√©n√©rer un rapport HTML pour une analyse d√©taill√©e
pytest --cov=. --cov-report=html
```

#### 2. Exemples de Tests Manquants

**Fonction √† tester :**
```python
def calculate_discount(price, discount_percent):
    if discount_percent < 0 or discount_percent > 100:
        raise ValueError("Discount must be between 0 and 100")
    return price * (1 - discount_percent / 100)
```

**Tests manquants :**
```python
import pytest

@pytest.mark.parametrize("price, discount, expected", [
    (100, 10, 90.0),     # R√©duction de 10%
    (100, 0, 100.0),     # Aucune r√©duction
    (100, 100, 0.0),     # 100% de r√©duction
    (100, 50, 50.0),     # 50% de r√©duction
    (0, 10, 0.0),        # Prix √† z√©ro
])
def test_calculate_discount(price, discount, expected):
    assert calculate_discount(price, discount) == expected

# Tester les cas d'erreur
@pytest.mark.parametrize("discount", [-1, 101])
def test_calculate_discount_invalid(discount):
    with pytest.raises(ValueError):
        calculate_discount(100, discount)
```

#### 3. Exclure du Code D√©lib√©r√©ment

```python
# Exclure une fonction sp√©cifique
def experimental_feature():  # pragma: no cover
    """Fonction exp√©rimentale non encore test√©e."""
    pass

# Exclure un bloc de code
if DEBUG_MODE:  # pragma: no cover
    logger.warning("Mode d√©bogage activ√©")
```

### Int√©gration Continue

Configuration recommand√©e pour GitHub Actions (`.github/workflows/tests.yml`) :

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports: [6379:6379]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run tests with coverage
      env:
        REDIS_URL: redis://localhost:6379/0
      run: |
        pytest --cov=. --cov-report=xml --cov-fail-under=70
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        fail_ci_if_error: true
```

### Outils Recommand√©s

- **Codecov** : Suivi de la couverture dans le temps
- **SonarQube** : Analyse statique et m√©triques de qualit√©
- **Pylint** : V√©rification de la qualit√© du code
- **Black** : Formatage automatique du code

### 2. Structure AAA (Arrange-Act-Assert)

```python
def test_webhook_config_update():
    # Arrange : pr√©parer les donn√©es
    config = {'webhook_url': 'https://example.com/hook'}
    
    # Act : ex√©cuter l'action
    result = update_webhook_config(config)
    
    # Assert : v√©rifier le r√©sultat
    assert result['success'] is True
```

### 3. Utiliser les fixtures

```python
# Plut√¥t que de cr√©er manuellement des mocks
def test_with_fixture(mock_logger, temp_file):
    # Les fixtures sont automatiquement inject√©es
    ...
```

### 4. Isolation des tests

```python
# Chaque test doit √™tre ind√©pendant
@pytest.fixture(autouse=True)
def reset_state():
    # Setup
    yield
    # Cleanup automatique apr√®s chaque test
```

### 5. Mocking appropri√©

```python
# Mock uniquement les d√©pendances externes
with patch('requests.post') as mock_post:
    mock_post.return_value.status_code = 200
    result = send_webhook(...)
    assert result is True
```

### 6. Singletons des services (tests)

- Certains services sont des Singletons (ex: `RuntimeFlagsService`, `WebhookConfigService`).
- Avant/Apr√®s un test, vous pouvez r√©initialiser l'instance pour isoler l'√©tat:

```python
from services.runtime_flags_service import RuntimeFlagsService
from services.webhook_config_service import WebhookConfigService

def setup_function():
    RuntimeFlagsService.reset_instance()
    WebhookConfigService.reset_instance()
```

- Utilisez des fichiers temporaires (`tmp_path`) pour la persistence JSON dans les tests.

### 7. Strat√©gie API‚Äëfirst

- Privil√©gier la validation par les endpoints (GET/POST) qui consomment les services.
- Exemple: mettre √† jour des flags via `POST /api/update_runtime_flags` puis v√©rifier `GET /api/get_runtime_flags`.
- Pour la config webhook, tester la validation HTTPS et le masquage d'URL via `GET/POST /api/webhooks/config`.

## Fixtures communes

D√©finies dans `tests/conftest.py` :

- `mock_redis` : Client Redis mock√© (fakeredis)
- `mock_logger` : Logger mock√©
- `temp_file` : Fichier temporaire
- `temp_dir` : R√©pertoire temporaire
- `flask_app` : Instance Flask pour tests
- `flask_client` : Client de test Flask
- `authenticated_flask_client` : Client Flask authentifi√©
- `sample_email_body` : Corps d'email exemple
- `sample_email_subject` : Sujet d'email exemple

## Debugging

### Afficher les logs

```bash
# Afficher tous les logs
pytest -s --log-cli-level=DEBUG

# Afficher seulement les logs d'un module
pytest -s --log-cli-level=DEBUG -k "test_webhook"
```

### Arr√™ter au premier √©chec

```bash
pytest -x
```

### Lancer le debugger au premier √©chec

```bash
pytest --pdb
```

### R√©-ex√©cuter seulement les tests √©chou√©s

```bash
# Premier run
pytest

# R√©-ex√©cuter seulement les √©checs
pytest --lf

# R√©-ex√©cuter les √©checs puis tous les autres
pytest --ff
```

## CI/CD

### Configuration recommand√©e

```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install -r requirements-dev.txt
    
    - name: Run tests with coverage
      run: |
        pytest --cov=. --cov-report=xml --cov-report=term
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

### Pr√©-commit hooks (optionnel)

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: pytest
        language: system
        pass_filenames: false
        always_run: true
```

## Checklist avant mise en production

Avant de d√©ployer en production, v√©rifier :

- [ ] Tous les tests passent : `pytest`
- [ ] Couverture ‚â• 75% : `pytest --cov=. --cov-report=term`
- [ ] Pas de tests skipp√©s non intentionnels
- [ ] Tests d'int√©gration passent avec config r√©elle
- [ ] Documentation √† jour
- [ ] Variables d'environnement document√©es
- [ ] Logs de debug d√©sactiv√©s en production

## Commandes utiles

```bash
# Suite compl√®te optimale
pytest -v --cov=. --cov-report=html --cov-report=term-missing -n auto

# Tests rapides (skip slow et external)
pytest -m "not slow and not imap and not redis"

# Tests critiques avant commit
pytest -m "unit or integration" --cov=. --cov-report=term

# V√©rifier la syntaxe sans ex√©cuter
pytest --collect-only

# Statistiques de la suite de tests
pytest --co -q
```

## Ressources

- [Documentation pytest](https://docs.pytest.org/)
- [pytest-cov documentation](https://pytest-cov.readthedocs.io/)
- [pytest-flask documentation](https://pytest-flask.readthedocs.io/)
- [Best practices pytest](https://docs.pytest.org/en/stable/goodpractices.html)

---

**Note** : Ce guide est vivant et doit √™tre mis √† jour lors de l'ajout de nouveaux types de tests ou pratiques.
